{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0_Colab_Memorry_Check.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiJahanbakht/cat-dog-classification-on-colab/blob/master/0_Colab_Memorry_Check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KDYS3j-2NZYG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "qsTfpnECF1Ud",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Check Colab available memory\n",
        "This snippet shows you how much free memory do you have to use Colab's **Tesla K80 GPU**.\n",
        "If the amount of used memory is more than 0%, it may be because of the other codes that you have run and are continuig to run in the background. If that's not the case, you can do two thing to get the most out of google's free memory.\n",
        "you can either run the code `!pkill -9 -f ipykernel_launcher` or from the `Runtime` menu select `Rset all runtimes...` and then click yes. After that reconnect to Colab VM from toolbar, and then run the following code. to see your available memory."
      ]
    },
    {
      "metadata": {
        "id": "UR-otZYKM5KP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> **Caution:**\n",
        "First check that you are actually using **gpu supporting ** **Colab** note book.  \n",
        "To do so click on `Runtime>Change runtime type` then select **Python3** from `[Runtime Type]` and **GPU** from `[Hardware accelerator]`."
      ]
    },
    {
      "metadata": {
        "id": "xdWObkPYYuX7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Check GPU memory allocation"
      ]
    },
    {
      "metadata": {
        "id": "lc6neglMLfLc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0kS-BzUQYz7H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reset environment"
      ]
    },
    {
      "metadata": {
        "id": "MJ5AanfOMHNn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pkill -9 -f ipykernel_launcher"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}