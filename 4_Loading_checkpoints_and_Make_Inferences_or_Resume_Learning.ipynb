{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_Loading checkpoints and Make Inferences or Resume Learning",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiJahanbakht/cat-dog-classification-on-colab/blob/master/4_Loading_checkpoints_and_Make_Inferences_or_Resume_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "p0t6dn7h2oOv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Previousely saved checkpoints\n",
        "To do so we have to do as follows:\n",
        "1.  Reconstruct the model from the structure saved in the checkpoint\n",
        "2.  Load the state dict to the model  \n",
        "\n",
        "Then, resume learning or...\n",
        "\n",
        "3. Freeze the parameters and enter evaluation mode if you are loading the model for inference  \n",
        "\n",
        "The following block does this for you"
      ]
    },
    {
      "metadata": {
        "id": "ip0Jfgw96mtH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch, torchvision, os\n",
        "from torch import nn, optim\n",
        "from torchvision import models\n",
        "\n",
        "filepath = '/content/drive/My Drive/Deep_Learning/Cat_Dog_run/checkpoint.pth'\n",
        "for_eval = True\n",
        "\n",
        "# Use Gpu if it's available\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Download pre-trained model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define cost function aas the negative log likelihood loss. It is useful to train a classification problem with C classes\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Adam optimizer with learning rate of 0.003\n",
        "optimizer = None\n",
        "\n",
        "if os.path.isfile(filepath):\n",
        "  \n",
        "  checkpoint = torch.load(filepath,)\n",
        "  start_epoch = checkpoint['epoch']\n",
        "  \n",
        "  model.fc = checkpoint['model']\n",
        "  model.fc.load_state_dict(checkpoint['state_dict'])\n",
        "  \n",
        "  optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  \n",
        "  if for_eval == True:\n",
        "    for parameter in model.fc.parameters():\n",
        "        parameter.requires_grad = False\n",
        "    model.eval()\n",
        "  else:\n",
        "    model.train()\n",
        "\n",
        "  model.to(device)      \n",
        "else:\n",
        "  print(\"=> no checkpoint found at '{}'\".format(filepath))\n",
        "            \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gwxfp7WGBq03",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, you can run Learning block of previous notebook.\n",
        ">**Caution:** If you are planning for resume learning, don't forget to update `start_epoch` variable by its new value to resume your work properly  \n",
        ">**Note:** To just evalute the model you can use codes in the proceeding blocks"
      ]
    },
    {
      "metadata": {
        "id": "OwI4Ujp9C91X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "baseDir = '/content/drive/My Drive/Deep_Learning/'\n",
        "dataDir = baseDir + 'Datasets/Cat_Dog_data/'\n",
        "runDir = baseDir + 'Cat_Dog_run/'\n",
        "\n",
        "# we used batch size of 64 images for batch learning\n",
        "batch_size = 64\n",
        "\n",
        "# For test data we did'nt use any form of Data Augmentation and just used\n",
        "# resize and center crop to adjust our images for input\n",
        "test_transforms = transforms.Compose([transforms.Resize(255),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])])\n",
        "# Define train and test folders located on google drive\n",
        "testset = datasets.ImageFolder(dataDir+'test',transform=test_transforms)\n",
        "\n",
        "# Define batch loaders\n",
        "testloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Ugadmr_-5B2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_loss = 0\n",
        "accuracy = 0\n",
        "# Do not compute gradients. Speeds up the algorithm alot\n",
        "with torch.no_grad():\n",
        "    # Load test data\n",
        "    for inputs, labels in testloader:\n",
        "        # Move test tensors to device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Do the forward pass\n",
        "        logps = model.forward(inputs)\n",
        "        # Compute cost\n",
        "        batch_loss = criterion(logps, labels)\n",
        "        # Sum up all the costs\n",
        "        eval_loss += batch_loss.item()\n",
        "\n",
        "        # Calculate accuracy. As you may remember we done LogSoftmax\n",
        "        # in the last layer. So, to actual probabolities we should\n",
        "        # calculate exp of output values\n",
        "        ps = torch.exp(logps)\n",
        "        # Select the best class\n",
        "        top_p, top_class = ps.topk(1, dim=1)\n",
        "        # Reshape our labels to match top_class shape\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        # Change the type of equals to float tensor\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "# Keep track of test losses for each batch\n",
        "mean_eval_loss = eval_loss/len(testloader)\n",
        "mean_acc = accuracy/len(testloader)\n",
        "\n",
        "print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "      f\"Test loss: {mean_eval_loss:.3f}.. \"\n",
        "      f\"Test accuracy: {mean_acc:.3f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}